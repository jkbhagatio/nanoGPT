{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction & Overview\n",
    "\n",
    "This notebook builds nanoGPT piece-by-piece, trains on the entire works of Shakespeare, and generates some\n",
    "text-completion output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Set notebook settings and imports.\"\"\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from warnings import warn\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.parametrizations import weight_norm\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Set environment variables for CUDA debugging.\"\"\"\n",
    "\n",
    "# Only uncomment to force synchronous CUDA operations for debugging\n",
    "# %env CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=device(type='cuda')\n",
      "\n",
      "Current GPU device: NVIDIA GeForce RTX 3090\n",
      "\n",
      "All GPU devices:\n",
      "Device 0: NVIDIA GeForce RTX 3090\n",
      "Device 1: NVIDIA GeForce RTX 2060\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Set torch device.\"\"\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"{device=}\\n\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current GPU device: {torch.cuda.get_device_name(device)}\\n\")\n",
    "    n_devices = torch.cuda.device_count()\n",
    "    print(\"All GPU devices:\")\n",
    "    for i in range(n_devices):\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Read in data.\"\"\"\n",
    "\n",
    "filepath = Path.cwd() / \"data/tiny_shakespeare.txt\"\n",
    "with open(filepath) as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_chars=1115394\n",
      "\n",
      "vocab_sz=65\n",
      "\n",
      "Tokens: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "\n",
      "First 100 chars of text:\n",
      "---\n",
      "\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "\"\"\"View some info about text.\"\"\"\n",
    "\n",
    "n_chars = len(text)\n",
    "tokens = sorted(set(text))\n",
    "vocab_sz = len(tokens)\n",
    "\n",
    "print(f\"{n_chars=}\")\n",
    "print(f\"\\n{vocab_sz=}\")\n",
    "print(f\"\\nTokens: {''.join(tokens)}\")\n",
    "print(f\"\\nFirst 100 chars of text:\\n---\\n\\n{text[:100]}\\n\\n---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 6, 1, 61, 53, 56, 50, 42, 2]\n",
      "Hello, world!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Tokenize and save tokens to a file.\"\"\"\n",
    "\n",
    "token_to_int = {t: i for i, t in enumerate(tokens)}\n",
    "int_to_token = {i: t for t, i in token_to_int.items()}\n",
    "encode = lambda tokens: [token_to_int[t] for t in tokens]\n",
    "decode = lambda ints: \"\".join([int_to_token[i] for i in ints])\n",
    "\n",
    "# Example\n",
    "print(encode(\"Hello, world!\"))\n",
    "print(decode(encode(\"Hello, world!\")))\n",
    "\n",
    "# Encode entire text dataset\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# Save tokens to file\n",
    "filepath = Path.cwd() / \"data/tiny_shakespeare_tokens.txt\"\n",
    "with open(filepath, \"w\") as f:\n",
    "    for tok in tokens:\n",
    "        f.write(\"%s\" % tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w=tensor([[0., -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf],\n",
      "        [0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0.]])\n",
      "w=tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500]])\n",
      "x=tensor([[ 0.1234,  0.0689, -1.8504, -0.5551],\n",
      "        [ 1.0161,  0.4646,  0.3244, -0.2380],\n",
      "        [-0.0536,  1.3074,  0.8214, -1.3427],\n",
      "        [ 0.9463,  1.3927,  0.7936, -0.6684]])\n",
      "attn_out=tensor([[ 0.1234,  0.0689, -1.8504, -0.5551],\n",
      "        [ 0.5698,  0.2668, -0.7630, -0.3965],\n",
      "        [ 0.3620,  0.6137, -0.2349, -0.7119],\n",
      "        [ 0.5081,  0.8084,  0.0222, -0.7010]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Motivating self-attention.\"\"\"\n",
    "\n",
    "# We can get weighted aggregations of past elements by matmul of `x` by a lower triangular matrix (weights)\n",
    "\n",
    "ctx_len = 4  # context length\n",
    "\n",
    "x = torch.randn(ctx_len, ctx_len)  # input sequence\n",
    "w = torch.zeros(ctx_len, ctx_len)  # attention weights\n",
    "tril = torch.tril(torch.ones(ctx_len, ctx_len))  # triangular mask for our weights\n",
    "w = w.masked_fill(tril == 0, float(\"-inf\"))  # mask out upper triangle (we can't access future info)\n",
    "print(f\"{w=}\")\n",
    "w = F.softmax(w, dim=1)  # convert weight values to probs\n",
    "print(f\"{w=}\")\n",
    "print(f\"{x=}\")\n",
    "attn_out = w @ x  # weighted aggregation (sum) of past elements (can think of this as self-attn output)\n",
    "print(f\"{attn_out=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_q_sim.shape=torch.Size([4, 4, 4])\n",
      "k_q_sim=tensor([[[ 0.3487, -0.1135, -0.0213,  0.0302],\n",
      "         [ 0.0026, -0.0246,  0.1513,  0.0830],\n",
      "         [ 0.0861, -0.1113,  0.5255,  0.2975],\n",
      "         [-0.3584,  0.1128,  0.0470, -0.0173]],\n",
      "\n",
      "        [[ 0.4830,  0.3114,  0.3637, -0.4686],\n",
      "         [ 0.0928,  0.0720,  0.0882, -0.1159],\n",
      "         [ 0.0366,  0.1030,  0.1473, -0.2049],\n",
      "         [-0.0879, -0.1774, -0.2481,  0.3428]],\n",
      "\n",
      "        [[-0.1646,  0.0421,  0.0096,  0.0463],\n",
      "         [-0.3919,  0.4353,  0.1338,  0.7327],\n",
      "         [-0.1373, -0.4617, -0.1563, -0.8837],\n",
      "         [-0.8719,  1.2711,  0.3978,  2.1920]],\n",
      "\n",
      "        [[ 0.1248,  0.3058, -0.1433,  0.3543],\n",
      "         [ 0.0079, -0.0271,  0.0883,  0.0300],\n",
      "         [ 0.1354,  0.3058, -0.1007,  0.3888],\n",
      "         [-0.2005, -0.4268,  0.0951, -0.5797]]], grad_fn=<DivBackward0>)\n",
      "attn_weights.shape=torch.Size([4, 4, 4])\n",
      "attn_weights=tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5068, 0.4932, 0.0000, 0.0000],\n",
      "         [0.2965, 0.2434, 0.4601, 0.0000],\n",
      "         [0.1815, 0.2908, 0.2723, 0.2553]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5052, 0.4948, 0.0000, 0.0000],\n",
      "         [0.3139, 0.3355, 0.3506, 0.0000],\n",
      "         [0.2323, 0.2124, 0.1979, 0.3574]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3042, 0.6958, 0.0000, 0.0000],\n",
      "         [0.3698, 0.2674, 0.3628, 0.0000],\n",
      "         [0.0290, 0.2471, 0.1032, 0.6207]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5088, 0.4912, 0.0000, 0.0000],\n",
      "         [0.3361, 0.3985, 0.2654, 0.0000],\n",
      "         [0.2614, 0.2084, 0.3513, 0.1789]]], grad_fn=<SoftmaxBackward0>)\n",
      "attn_out.shape=torch.Size([4, 4, 2])\n",
      "attn_out=tensor([[[-1.0552, -1.1140],\n",
      "         [-0.6415, -0.7954],\n",
      "         [-0.5013, -0.3108],\n",
      "         [-0.2122, -0.2490]],\n",
      "\n",
      "        [[-0.5576, -0.4369],\n",
      "         [-0.2023, -0.4284],\n",
      "         [-0.1361, -0.3964],\n",
      "         [-0.2542, -0.0174]],\n",
      "\n",
      "        [[ 0.0501, -0.4860],\n",
      "         [ 0.1701,  0.3058],\n",
      "         [-0.0449,  0.0724],\n",
      "         [ 0.6225,  0.0047]],\n",
      "\n",
      "        [[-1.0106,  0.0956],\n",
      "         [-0.4313, -0.1058],\n",
      "         [-0.2925, -0.1532],\n",
      "         [-0.3044, -0.2114]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Self-attention for a Decoder.\"\"\"\n",
    "\n",
    "# (Note: here we use a mask to prevent the Decoder from looking ahead (into future) in the sequence.\n",
    "# This is not necessary for an Encoder.)\n",
    "\n",
    "# <s Create layers for computing Query, Key, Value tensors\n",
    "batch_sz, ctx_len, emb_dim = 4, 4, 8\n",
    "# The head size is the dimensionality of the Query, Key, and Value tensors (previously we assumed\n",
    "# this was just equal to `ctx_len`). Typically, we can think of `emb_dim` as getting fanned into\n",
    "# a smaller `head_sz` dimensionality. The greater the `head_sz`, the more granular the information\n",
    "# a head can capture.\n",
    "head_sz = 2\n",
    "# We can think of input `x` into query, key, and value layers with a rough analogy:\n",
    "# For a given token, we can think of `x` as its private information, it's query as what it wants to know \n",
    "# about other tokens, the keys (of itself and all other tokens) as the summary of the tokens, and the \n",
    "# values as the detailed information / content about the tokens.\n",
    "query = nn.Linear(emb_dim, head_sz, bias=False)\n",
    "key = nn.Linear(emb_dim, head_sz, bias=False)\n",
    "value = nn.Linear(emb_dim, head_sz, bias=False)\n",
    "# (Note: we don't use biases for these layers because we want to ensure that the dot product of the\n",
    "# query and key tensors is the only thing that determines the similarity between the query and key,\n",
    "# to determine the weighting of values (i.e. the attention out))\n",
    "# /s>\n",
    "\n",
    "# <s Get Query, Key, Value tensors\n",
    "x = torch.randn(batch_sz, ctx_len, emb_dim)\n",
    "q = query(x)  # -> [batch_sz, ctx_len, head_sz]\n",
    "k = key(x)  \n",
    "v = value(x)\n",
    "# /s>\n",
    "\n",
    "# <s Get Key-Query attention weights\n",
    "# First we compute the similarity between each token's query and all other tokens' keys as the dot \n",
    "# across `emb_dim` for each batch example over `ctx_len`, scaled by `head_sz` to preserve k, q variance.\n",
    "k_q_sim = q @ k.transpose(2, 1) / np.sqrt(head_sz)\n",
    "print(f\"{k_q_sim.shape=}\")\n",
    "print(f\"{k_q_sim=}\")\n",
    "tril = torch.tril(torch.ones(ctx_len, ctx_len))  # mask out upper triangle (we can't access future info)\n",
    "k_q_sim = k_q_sim.masked_fill(tril == 0, float(\"-inf\"))\n",
    "attn_weights = F.softmax(k_q_sim, dim=2)\n",
    "print(f\"{attn_weights.shape=}\")\n",
    "print(f\"{attn_weights=}\")\n",
    "# /s>\n",
    "attn_out = attn_weights @ v  # weighted sum of values\n",
    "print(f\"{attn_out.shape=}\")\n",
    "print(f\"{attn_out=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create a class for a head of a self-attention unit for a Decoder.\"\"\"\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"Self-attention head.\"\"\"\n",
    "\n",
    "    def __init__(self, head_sz, emb_dim):\n",
    "        \"\"\"Initialize key, query, value.\"\"\"\n",
    "        super().__init__()\n",
    "        self.head_sz, self.emb_dim = head_sz, emb_dim\n",
    "        self.key = nn.Linear(emb_dim, head_sz, bias=False)\n",
    "        self.query = nn.Linear(emb_dim, head_sz, bias=False)\n",
    "        self.value = nn.Linear(emb_dim, head_sz, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Compute self-attention output.\"\"\"\n",
    "        _batch_sz, ctx_len, _emb_dim = x.shape\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)  # -> [batch_sz, ctx_len, head_sz]\n",
    "        v = self.value(x)\n",
    "        k_q_sim = q @ k.transpose(2, 1) / np.sqrt(self.head_sz)  # scaled attention to preserve k, q variance\n",
    "        tril = torch.tril(torch.ones(ctx_len, ctx_len)).to(device)  # mask out upper triangle (we can't access future info)\n",
    "        k_q_sim = k_q_sim.masked_fill(tril == 0, float(\"-inf\"))\n",
    "        attn_weights = F.softmax(k_q_sim, dim=2)\n",
    "        attn_out = attn_weights @ v  # weighted sum of values\n",
    "        # Note, if *not* using this in a MultiHead setting, we should project back to emb_dim\n",
    "        #proj = nn.Linear(head_sz, emb_dim)\n",
    "        #attn_out = proj(attn_out)\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_attn_out.shape=torch.Size([4, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Multi-head self-attention for a Decoder.\"\"\"\n",
    "\n",
    "# Multi-head attention is applying multiple self-attention heads in parallel, then concatenating their\n",
    "# outputs, and projecting them back to `emb_dim`. This allows the model to attend to information from\n",
    "# different subspaces (representations) of the input simultaneously.\n",
    "\n",
    "n_heads = 2\n",
    "heads = nn.ModuleList([Head(head_sz, emb_dim) for _ in range(n_heads)]).to(device)\n",
    "attn_outs = [head(x.to(device)) for head in heads]  # -> n_heads x [batch_sz, ctx_len, head_sz]\n",
    "attn_out = torch.cat(attn_outs, dim=2)  # -> [batch_sz, ctx_len, n_heads * head_sz]\n",
    "proj = nn.Linear(n_heads * head_sz, emb_dim).to(device)  # project back to emb_dim\n",
    "multi_attn_out = proj(attn_out)\n",
    "print(f\"{multi_attn_out.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create a class for multi-head self-attention for a Decoder.\"\"\"\n",
    "\n",
    "class MultiHead(nn.Module):\n",
    "    \"\"\"Multi-head self-attention.\"\"\"\n",
    "\n",
    "    def __init__(self, n_heads, head_sz, emb_dim):\n",
    "        \"\"\"Initialize heads.\"\"\"\n",
    "        super().__init__()\n",
    "        self.n_heads, self.head_sz, self.emb_dim = n_heads, head_sz, emb_dim\n",
    "        self.heads = nn.ModuleList([Head(head_sz, emb_dim) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(self.n_heads * self.head_sz, self.emb_dim)  # project back to `emb_dim`\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_outs = [head(x) for head in self.heads]\n",
    "        attn_out = torch.cat(attn_outs, dim=2)  # concatenate across head dimension\n",
    "        attn_out = self.proj(attn_out)\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create a class for a Feedforward network that operates on attention outputs.\"\"\"\n",
    "\n",
    "# Self-attention operates on the entire input sequence at once, but feedforward layer(s) can be applied\n",
    "# independently to each token's representation from the self-attention layer to allow the model to\n",
    "# process and adjust features of each token individually, that might otherwise get diluted from the global\n",
    "# attention mechanism.\n",
    "\n",
    "class Feedforward(nn.Module):\n",
    "    \"\"\"Feedforward layer.\"\"\"\n",
    "\n",
    "    def __init__(self, emb_dim, ff_dim):\n",
    "        \"\"\"Initialize weights.\"\"\"\n",
    "        super().__init__()\n",
    "        # Linear layer ReLU sandwich: dim fans out by factor of `ff_dim` and then back to `emb_dim`.\n",
    "        # (\"Position-wise Feed-Forward Networks\" in \"Attention is All You Need\")\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim * ff_dim), nn.ReLU(), nn.Linear(emb_dim * ff_dim, emb_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create a Transformer block: communication -> computation (self-attention + feedforward + res + norm).\"\"\"\n",
    "\n",
    "# Parts:\n",
    "#  - Multi-head self-attention\n",
    "#  - Position-wise feedforward network\n",
    "#  - Residual connections\n",
    "#  - Layer normalization (pre-norm formulation)\n",
    "#  - ~ Weight normalization ~ (not for now)\n",
    "#  - Dropout\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block: communication followed by computation.\"\"\"\n",
    "\n",
    "    def __init__(self, n_heads, head_sz, emb_dim, ff_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.n_heads, self.head_sz, self.emb_dim, self.ff_dim = n_heads, head_sz, emb_dim, ff_dim\n",
    "        self.self_attn_ln = nn.LayerNorm(emb_dim)  # layer norm pre self-attention\n",
    "        self.self_attn = MultiHead(n_heads, head_sz, emb_dim)  # multi-head self-attention\n",
    "        self.self_attn_dropout = nn.Dropout(dropout)  # dropout after self-attention\n",
    "        self.ff_ln = nn.LayerNorm(emb_dim)  # layer norm pre feedforward\n",
    "        self.ff = Feedforward(emb_dim, ff_dim)  # position-wise feedforward\n",
    "        self.ff_dropout = nn.Dropout(dropout)  # dropout after feedforward\n",
    "\n",
    "    def forward(self, x):\n",
    "        # layer-norm -> self-attention -> dropout + residual\n",
    "        x = x + self.self_attn_dropout(self.self_attn(self.self_attn_ln(x)))\n",
    "        # layer-norm -> feedforward -> dropout + residual\n",
    "        x = x + self.ff_dropout(self.ff(self.ff_ln(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create NanoGPT: Decoder-only Transformer.\"\"\"\n",
    "\n",
    "# In addition to our Transformer blocks, we need token embedding and positional embedding layers, to compute\n",
    "# the positional encodings that get passed to the attention units in the transformer blocks.\n",
    "\n",
    "# We'll also apply weight init.\n",
    "\n",
    "# We want our output to be [batch_sz, ctx_len, n_tokens], because we want to predict the next token for each \n",
    "# token in the context.\n",
    "\n",
    "\n",
    "class NanoGPT(nn.Module):\n",
    "    \"\"\"NanoGPT: Decoder-only Transformer.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_tokens=vocab_sz,\n",
    "        ctx_len=512,\n",
    "        n_blocks=8,\n",
    "        n_heads=10,\n",
    "        head_sz=64,\n",
    "        emb_dim=512,\n",
    "        ff_dim=4,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        (\n",
    "            self.n_tokens,\n",
    "            self.ctx_len,\n",
    "            self.n_blocks,\n",
    "            self.n_heads,\n",
    "            self.head_sz,\n",
    "            self.emb_dim,\n",
    "            self.ff_dim,\n",
    "        ) = (n_tokens, ctx_len, n_blocks, n_heads, head_sz, emb_dim, ff_dim)\n",
    "        if (emb_dim / n_heads / head_sz) != 1:\n",
    "            warn(f\"Ratio of n_heads and head_sz to emb_dim ({emb_dim / n_heads / head_sz}) is not 1\")\n",
    "        self.tok_emb = nn.Embedding(n_tokens, emb_dim)  # to learn token embeddings\n",
    "        self.pos_emb = nn.Embedding(ctx_len, emb_dim)  # to learn positional embeddings\n",
    "        self.blocks = nn.Sequential(  # Transformer blocks\n",
    "            *[Block(n_heads, head_sz, emb_dim, ff_dim, dropout) for _ in range(n_blocks)]\n",
    "        )\n",
    "        self.f_ln = nn.LayerNorm(emb_dim)  # final layer norm\n",
    "        self.f_dropout = nn.Dropout(dropout)  # final dropout\n",
    "        self.out = nn.Linear(emb_dim, n_tokens)\n",
    "        self.apply(self.xavier_init)\n",
    "\n",
    "    @staticmethod\n",
    "    def xavier_init(module, gain=1):\n",
    "        \"\"\"Applies Xavier initialization to all linear and embedding layer weights.\"\"\"\n",
    "        if isinstance(module, nn.Linear) or isinstance(module, nn.Embedding):\n",
    "            nn.init.xavier_normal_(module.weight, gain=gain)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_sz, ctx_len = x.shape\n",
    "        # Compute positional encodings\n",
    "        tok_emb = self.tok_emb(x)  # -> [batch_sz, ctx_len, emb_dim]\n",
    "        pos_emb = self.pos_emb.weight[0:ctx_len]  # -> [ctx_len, emb_dim]\n",
    "        pos_enc = tok_emb + pos_emb  # -> [batch_sz, ctx_len, emb_dim]\n",
    "        # Go through transformer blocks and final linear layer\n",
    "        logits = self.out(self.f_dropout(self.f_ln(self.blocks(pos_enc))))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create a function for generating output from the model.\"\"\"\n",
    "\n",
    "def generate(model, tokens, in_txt=None, n_tokens=100, temp=1.0, top_k=None, seed=42, print_gen=True):\n",
    "    \"\"\"Generate text from a nanoGPT model.\"\"\"\n",
    "    # Set a random seed for generation\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Create token_to_int, int_to_token dicts.\n",
    "    token_to_int = {t: i for i, t in enumerate(tokens)}\n",
    "    int_to_token = {i: t for t, i in token_to_int.items()}\n",
    "\n",
    "    # Process input_text if provided, else start with \"\\n\".\n",
    "    if in_txt is not None:\n",
    "        # Convert input text to tokens and encode.\n",
    "        encode = lambda tokens: [token_to_int[t] for t in tokens]\n",
    "        in_tkns = encode(in_txt)\n",
    "        input_len = len(in_tkns)\n",
    "        # Initialize output starting with input text.\n",
    "        x = torch.zeros((input_len + n_tokens,), dtype=torch.long).to(device)\n",
    "        x[:input_len] = torch.tensor(in_tkns, dtype=torch.long).to(device)\n",
    "    else:\n",
    "        # Initialize output starting with \"\\n\".\n",
    "        x = torch.zeros((1 + n_tokens,), dtype=torch.long).to(device)\n",
    "        x[0] = token_to_int[\"\\n\"]\n",
    "        input_len = 1\n",
    "    assert len(x) <= model.ctx_len, (\n",
    "        f\"Generated length {len(x) + n_tokens} would exceed model context length {model.ctx_len}.\"\n",
    "    )\n",
    "\n",
    "    # Run inference (generation) in eval mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        first_gen_idx, last_gen_idx = input_len - 1, input_len + n_tokens - 1\n",
    "        for i in range(first_gen_idx, last_gen_idx):  # start gen after `input_len`\n",
    "            model_first_ctx = 0 if i < model.ctx_len else i - model.ctx_len + 1\n",
    "            logits = model(x[model_first_ctx:(i + 1)].unsqueeze(0))  # feed in `x` with a batch_sz of 1\n",
    "            # Get logits for just `len(tokens)` (squeeze out ctx_len), and scale by temp\n",
    "            logits = logits[:, -1, :] / temp\n",
    "            if top_k is not None:  # limit to top_k most likely tokens\n",
    "                top_vals, top_idxs = logits.topk(top_k, dim=1)\n",
    "                probs = F.softmax(top_vals, dim=1)  # compute top_k probs\n",
    "                next_tkn_int = top_idxs.gather(1, torch.multinomial(probs, 1))  # sample top_k probs\n",
    "            else:\n",
    "                probs = F.softmax(logits, dim=1)  # compute probs for all tokens\n",
    "                next_tkn_int = torch.multinomial(probs, 1)  # sample from probs\n",
    "            x[i + 1] = next_tkn_int\n",
    "            if print_gen:\n",
    "                print(int_to_token[next_tkn_int.item()], end=\"\")\n",
    "\n",
    "    # Decode `x` and return it.\n",
    "    decode = lambda ints: \"\".join([int_to_token[i] for i in ints])\n",
    "    return decode(x.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jai\\AppData\\Local\\Temp\\ipykernel_41960\\2513295627.py:37: UserWarning: Ratio of n_heads and head_sz to emb_dim 0.8 is not 1\n",
      "  warn(f\"Ratio of n_heads and head_sz to emb_dim {emb_dim / n_heads / head_sz} is not 1\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NanoGPT(\n",
      "  (tok_emb): Embedding(65, 512)\n",
      "  (pos_emb): Embedding(512, 512)\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (self_attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (self_attn): MultiHead(\n",
      "        (heads): ModuleList(\n",
      "          (0-9): 10 x Head(\n",
      "            (key): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (query): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=512, out_features=64, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=640, out_features=512, bias=True)\n",
      "      )\n",
      "      (self_attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (ff_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (ff): Feedforward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (ff_dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): Block(\n",
      "      (self_attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (self_attn): MultiHead(\n",
      "        (heads): ModuleList(\n",
      "          (0-9): 10 x Head(\n",
      "            (key): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (query): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=512, out_features=64, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=640, out_features=512, bias=True)\n",
      "      )\n",
      "      (self_attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (ff_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (ff): Feedforward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (ff_dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): Block(\n",
      "      (self_attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (self_attn): MultiHead(\n",
      "        (heads): ModuleList(\n",
      "          (0-9): 10 x Head(\n",
      "            (key): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (query): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=512, out_features=64, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=640, out_features=512, bias=True)\n",
      "      )\n",
      "      (self_attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (ff_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (ff): Feedforward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (ff_dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (3): Block(\n",
      "      (self_attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (self_attn): MultiHead(\n",
      "        (heads): ModuleList(\n",
      "          (0-9): 10 x Head(\n",
      "            (key): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (query): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=512, out_features=64, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=640, out_features=512, bias=True)\n",
      "      )\n",
      "      (self_attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (ff_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (ff): Feedforward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (ff_dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (4): Block(\n",
      "      (self_attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (self_attn): MultiHead(\n",
      "        (heads): ModuleList(\n",
      "          (0-9): 10 x Head(\n",
      "            (key): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (query): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=512, out_features=64, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=640, out_features=512, bias=True)\n",
      "      )\n",
      "      (self_attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (ff_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (ff): Feedforward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (ff_dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (5): Block(\n",
      "      (self_attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (self_attn): MultiHead(\n",
      "        (heads): ModuleList(\n",
      "          (0-9): 10 x Head(\n",
      "            (key): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (query): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=512, out_features=64, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=640, out_features=512, bias=True)\n",
      "      )\n",
      "      (self_attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (ff_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (ff): Feedforward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (ff_dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (6): Block(\n",
      "      (self_attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (self_attn): MultiHead(\n",
      "        (heads): ModuleList(\n",
      "          (0-9): 10 x Head(\n",
      "            (key): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (query): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=512, out_features=64, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=640, out_features=512, bias=True)\n",
      "      )\n",
      "      (self_attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (ff_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (ff): Feedforward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (ff_dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (7): Block(\n",
      "      (self_attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (self_attn): MultiHead(\n",
      "        (heads): ModuleList(\n",
      "          (0-9): 10 x Head(\n",
      "            (key): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (query): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=512, out_features=64, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=640, out_features=512, bias=True)\n",
      "      )\n",
      "      (self_attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (ff_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (ff): Feedforward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (ff_dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (f_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (f_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (out): Linear(in_features=512, out_features=65, bias=True)\n",
      ")\n",
      "name='tok_emb.weight': n_params=33280\n",
      "name='pos_emb.weight': n_params=262144\n",
      "name='blocks.0.self_attn_ln.weight': n_params=512\n",
      "name='blocks.0.self_attn_ln.bias': n_params=512\n",
      "name='blocks.0.self_attn.heads.0.key.weight': n_params=32768\n",
      "name='blocks.0.self_attn.heads.0.query.weight': n_params=32768\n",
      "name='blocks.0.self_attn.heads.0.value.weight': n_params=32768\n",
      "name='blocks.0.self_attn.heads.1.key.weight': n_params=32768\n",
      "name='blocks.0.self_attn.heads.1.query.weight': n_params=32768\n",
      "name='blocks.0.self_attn.heads.1.value.weight': n_params=32768\n",
      "name='blocks.0.self_attn.heads.2.key.weight': n_params=32768\n",
      "name='blocks.0.self_attn.heads.2.query.weight': n_params=32768\n",
      "name='blocks.0.self_attn.heads.2.value.weight': n_params=32768\n",
      "name='blocks.0.self_attn.heads.3.key.weight': n_params=32768\n",
      "name='blocks.0.self_attn.heads.3.query.weight': n_params=32768\n",
      "name='blocks.0.self_attn.heads.3.value.weight': n_params=32768\n",
      "name='blocks.0.self_attn.heads.4.key.weight': n_params=32768\n",
      "name='blocks.0.self_attn.heads.4.query.weight': n_params=32768\n",
      "name='blocks.0.self_attn.heads.4.value.weight': n_params=32768\n",
      "name='blocks.0.self_attn.heads.5.key.weight': n_params=32768\n",
      "name='blocks.0.self_attn.heads.5.query.weight': n_params=32768\n",
      "name='blocks.0.self_attn.heads.5.value.weight': n_params=32768\n",
      "name='blocks.0.self_attn.heads.6.key.weight': n_params=32768\n",
      "name='blocks.0.self_attn.heads.6.query.weight': n_params=32768\n",
      "name='blocks.0.self_attn.heads.6.value.weight': n_params=32768\n",
      "name='blocks.0.self_attn.heads.7.key.weight': n_params=32768\n",
      "name='blocks.0.self_attn.heads.7.query.weight': n_params=32768\n",
      "name='blocks.0.self_attn.heads.7.value.weight': n_params=32768\n",
      "name='blocks.0.self_attn.heads.8.key.weight': n_params=32768\n",
      "name='blocks.0.self_attn.heads.8.query.weight': n_params=32768\n",
      "name='blocks.0.self_attn.heads.8.value.weight': n_params=32768\n",
      "name='blocks.0.self_attn.heads.9.key.weight': n_params=32768\n",
      "name='blocks.0.self_attn.heads.9.query.weight': n_params=32768\n",
      "name='blocks.0.self_attn.heads.9.value.weight': n_params=32768\n",
      "name='blocks.0.self_attn.proj.weight': n_params=327680\n",
      "name='blocks.0.self_attn.proj.bias': n_params=512\n",
      "name='blocks.0.ff_ln.weight': n_params=512\n",
      "name='blocks.0.ff_ln.bias': n_params=512\n",
      "name='blocks.0.ff.layers.0.weight': n_params=1048576\n",
      "name='blocks.0.ff.layers.0.bias': n_params=2048\n",
      "name='blocks.0.ff.layers.2.weight': n_params=1048576\n",
      "name='blocks.0.ff.layers.2.bias': n_params=512\n",
      "name='blocks.1.self_attn_ln.weight': n_params=512\n",
      "name='blocks.1.self_attn_ln.bias': n_params=512\n",
      "name='blocks.1.self_attn.heads.0.key.weight': n_params=32768\n",
      "name='blocks.1.self_attn.heads.0.query.weight': n_params=32768\n",
      "name='blocks.1.self_attn.heads.0.value.weight': n_params=32768\n",
      "name='blocks.1.self_attn.heads.1.key.weight': n_params=32768\n",
      "name='blocks.1.self_attn.heads.1.query.weight': n_params=32768\n",
      "name='blocks.1.self_attn.heads.1.value.weight': n_params=32768\n",
      "name='blocks.1.self_attn.heads.2.key.weight': n_params=32768\n",
      "name='blocks.1.self_attn.heads.2.query.weight': n_params=32768\n",
      "name='blocks.1.self_attn.heads.2.value.weight': n_params=32768\n",
      "name='blocks.1.self_attn.heads.3.key.weight': n_params=32768\n",
      "name='blocks.1.self_attn.heads.3.query.weight': n_params=32768\n",
      "name='blocks.1.self_attn.heads.3.value.weight': n_params=32768\n",
      "name='blocks.1.self_attn.heads.4.key.weight': n_params=32768\n",
      "name='blocks.1.self_attn.heads.4.query.weight': n_params=32768\n",
      "name='blocks.1.self_attn.heads.4.value.weight': n_params=32768\n",
      "name='blocks.1.self_attn.heads.5.key.weight': n_params=32768\n",
      "name='blocks.1.self_attn.heads.5.query.weight': n_params=32768\n",
      "name='blocks.1.self_attn.heads.5.value.weight': n_params=32768\n",
      "name='blocks.1.self_attn.heads.6.key.weight': n_params=32768\n",
      "name='blocks.1.self_attn.heads.6.query.weight': n_params=32768\n",
      "name='blocks.1.self_attn.heads.6.value.weight': n_params=32768\n",
      "name='blocks.1.self_attn.heads.7.key.weight': n_params=32768\n",
      "name='blocks.1.self_attn.heads.7.query.weight': n_params=32768\n",
      "name='blocks.1.self_attn.heads.7.value.weight': n_params=32768\n",
      "name='blocks.1.self_attn.heads.8.key.weight': n_params=32768\n",
      "name='blocks.1.self_attn.heads.8.query.weight': n_params=32768\n",
      "name='blocks.1.self_attn.heads.8.value.weight': n_params=32768\n",
      "name='blocks.1.self_attn.heads.9.key.weight': n_params=32768\n",
      "name='blocks.1.self_attn.heads.9.query.weight': n_params=32768\n",
      "name='blocks.1.self_attn.heads.9.value.weight': n_params=32768\n",
      "name='blocks.1.self_attn.proj.weight': n_params=327680\n",
      "name='blocks.1.self_attn.proj.bias': n_params=512\n",
      "name='blocks.1.ff_ln.weight': n_params=512\n",
      "name='blocks.1.ff_ln.bias': n_params=512\n",
      "name='blocks.1.ff.layers.0.weight': n_params=1048576\n",
      "name='blocks.1.ff.layers.0.bias': n_params=2048\n",
      "name='blocks.1.ff.layers.2.weight': n_params=1048576\n",
      "name='blocks.1.ff.layers.2.bias': n_params=512\n",
      "name='blocks.2.self_attn_ln.weight': n_params=512\n",
      "name='blocks.2.self_attn_ln.bias': n_params=512\n",
      "name='blocks.2.self_attn.heads.0.key.weight': n_params=32768\n",
      "name='blocks.2.self_attn.heads.0.query.weight': n_params=32768\n",
      "name='blocks.2.self_attn.heads.0.value.weight': n_params=32768\n",
      "name='blocks.2.self_attn.heads.1.key.weight': n_params=32768\n",
      "name='blocks.2.self_attn.heads.1.query.weight': n_params=32768\n",
      "name='blocks.2.self_attn.heads.1.value.weight': n_params=32768\n",
      "name='blocks.2.self_attn.heads.2.key.weight': n_params=32768\n",
      "name='blocks.2.self_attn.heads.2.query.weight': n_params=32768\n",
      "name='blocks.2.self_attn.heads.2.value.weight': n_params=32768\n",
      "name='blocks.2.self_attn.heads.3.key.weight': n_params=32768\n",
      "name='blocks.2.self_attn.heads.3.query.weight': n_params=32768\n",
      "name='blocks.2.self_attn.heads.3.value.weight': n_params=32768\n",
      "name='blocks.2.self_attn.heads.4.key.weight': n_params=32768\n",
      "name='blocks.2.self_attn.heads.4.query.weight': n_params=32768\n",
      "name='blocks.2.self_attn.heads.4.value.weight': n_params=32768\n",
      "name='blocks.2.self_attn.heads.5.key.weight': n_params=32768\n",
      "name='blocks.2.self_attn.heads.5.query.weight': n_params=32768\n",
      "name='blocks.2.self_attn.heads.5.value.weight': n_params=32768\n",
      "name='blocks.2.self_attn.heads.6.key.weight': n_params=32768\n",
      "name='blocks.2.self_attn.heads.6.query.weight': n_params=32768\n",
      "name='blocks.2.self_attn.heads.6.value.weight': n_params=32768\n",
      "name='blocks.2.self_attn.heads.7.key.weight': n_params=32768\n",
      "name='blocks.2.self_attn.heads.7.query.weight': n_params=32768\n",
      "name='blocks.2.self_attn.heads.7.value.weight': n_params=32768\n",
      "name='blocks.2.self_attn.heads.8.key.weight': n_params=32768\n",
      "name='blocks.2.self_attn.heads.8.query.weight': n_params=32768\n",
      "name='blocks.2.self_attn.heads.8.value.weight': n_params=32768\n",
      "name='blocks.2.self_attn.heads.9.key.weight': n_params=32768\n",
      "name='blocks.2.self_attn.heads.9.query.weight': n_params=32768\n",
      "name='blocks.2.self_attn.heads.9.value.weight': n_params=32768\n",
      "name='blocks.2.self_attn.proj.weight': n_params=327680\n",
      "name='blocks.2.self_attn.proj.bias': n_params=512\n",
      "name='blocks.2.ff_ln.weight': n_params=512\n",
      "name='blocks.2.ff_ln.bias': n_params=512\n",
      "name='blocks.2.ff.layers.0.weight': n_params=1048576\n",
      "name='blocks.2.ff.layers.0.bias': n_params=2048\n",
      "name='blocks.2.ff.layers.2.weight': n_params=1048576\n",
      "name='blocks.2.ff.layers.2.bias': n_params=512\n",
      "name='blocks.3.self_attn_ln.weight': n_params=512\n",
      "name='blocks.3.self_attn_ln.bias': n_params=512\n",
      "name='blocks.3.self_attn.heads.0.key.weight': n_params=32768\n",
      "name='blocks.3.self_attn.heads.0.query.weight': n_params=32768\n",
      "name='blocks.3.self_attn.heads.0.value.weight': n_params=32768\n",
      "name='blocks.3.self_attn.heads.1.key.weight': n_params=32768\n",
      "name='blocks.3.self_attn.heads.1.query.weight': n_params=32768\n",
      "name='blocks.3.self_attn.heads.1.value.weight': n_params=32768\n",
      "name='blocks.3.self_attn.heads.2.key.weight': n_params=32768\n",
      "name='blocks.3.self_attn.heads.2.query.weight': n_params=32768\n",
      "name='blocks.3.self_attn.heads.2.value.weight': n_params=32768\n",
      "name='blocks.3.self_attn.heads.3.key.weight': n_params=32768\n",
      "name='blocks.3.self_attn.heads.3.query.weight': n_params=32768\n",
      "name='blocks.3.self_attn.heads.3.value.weight': n_params=32768\n",
      "name='blocks.3.self_attn.heads.4.key.weight': n_params=32768\n",
      "name='blocks.3.self_attn.heads.4.query.weight': n_params=32768\n",
      "name='blocks.3.self_attn.heads.4.value.weight': n_params=32768\n",
      "name='blocks.3.self_attn.heads.5.key.weight': n_params=32768\n",
      "name='blocks.3.self_attn.heads.5.query.weight': n_params=32768\n",
      "name='blocks.3.self_attn.heads.5.value.weight': n_params=32768\n",
      "name='blocks.3.self_attn.heads.6.key.weight': n_params=32768\n",
      "name='blocks.3.self_attn.heads.6.query.weight': n_params=32768\n",
      "name='blocks.3.self_attn.heads.6.value.weight': n_params=32768\n",
      "name='blocks.3.self_attn.heads.7.key.weight': n_params=32768\n",
      "name='blocks.3.self_attn.heads.7.query.weight': n_params=32768\n",
      "name='blocks.3.self_attn.heads.7.value.weight': n_params=32768\n",
      "name='blocks.3.self_attn.heads.8.key.weight': n_params=32768\n",
      "name='blocks.3.self_attn.heads.8.query.weight': n_params=32768\n",
      "name='blocks.3.self_attn.heads.8.value.weight': n_params=32768\n",
      "name='blocks.3.self_attn.heads.9.key.weight': n_params=32768\n",
      "name='blocks.3.self_attn.heads.9.query.weight': n_params=32768\n",
      "name='blocks.3.self_attn.heads.9.value.weight': n_params=32768\n",
      "name='blocks.3.self_attn.proj.weight': n_params=327680\n",
      "name='blocks.3.self_attn.proj.bias': n_params=512\n",
      "name='blocks.3.ff_ln.weight': n_params=512\n",
      "name='blocks.3.ff_ln.bias': n_params=512\n",
      "name='blocks.3.ff.layers.0.weight': n_params=1048576\n",
      "name='blocks.3.ff.layers.0.bias': n_params=2048\n",
      "name='blocks.3.ff.layers.2.weight': n_params=1048576\n",
      "name='blocks.3.ff.layers.2.bias': n_params=512\n",
      "name='blocks.4.self_attn_ln.weight': n_params=512\n",
      "name='blocks.4.self_attn_ln.bias': n_params=512\n",
      "name='blocks.4.self_attn.heads.0.key.weight': n_params=32768\n",
      "name='blocks.4.self_attn.heads.0.query.weight': n_params=32768\n",
      "name='blocks.4.self_attn.heads.0.value.weight': n_params=32768\n",
      "name='blocks.4.self_attn.heads.1.key.weight': n_params=32768\n",
      "name='blocks.4.self_attn.heads.1.query.weight': n_params=32768\n",
      "name='blocks.4.self_attn.heads.1.value.weight': n_params=32768\n",
      "name='blocks.4.self_attn.heads.2.key.weight': n_params=32768\n",
      "name='blocks.4.self_attn.heads.2.query.weight': n_params=32768\n",
      "name='blocks.4.self_attn.heads.2.value.weight': n_params=32768\n",
      "name='blocks.4.self_attn.heads.3.key.weight': n_params=32768\n",
      "name='blocks.4.self_attn.heads.3.query.weight': n_params=32768\n",
      "name='blocks.4.self_attn.heads.3.value.weight': n_params=32768\n",
      "name='blocks.4.self_attn.heads.4.key.weight': n_params=32768\n",
      "name='blocks.4.self_attn.heads.4.query.weight': n_params=32768\n",
      "name='blocks.4.self_attn.heads.4.value.weight': n_params=32768\n",
      "name='blocks.4.self_attn.heads.5.key.weight': n_params=32768\n",
      "name='blocks.4.self_attn.heads.5.query.weight': n_params=32768\n",
      "name='blocks.4.self_attn.heads.5.value.weight': n_params=32768\n",
      "name='blocks.4.self_attn.heads.6.key.weight': n_params=32768\n",
      "name='blocks.4.self_attn.heads.6.query.weight': n_params=32768\n",
      "name='blocks.4.self_attn.heads.6.value.weight': n_params=32768\n",
      "name='blocks.4.self_attn.heads.7.key.weight': n_params=32768\n",
      "name='blocks.4.self_attn.heads.7.query.weight': n_params=32768\n",
      "name='blocks.4.self_attn.heads.7.value.weight': n_params=32768\n",
      "name='blocks.4.self_attn.heads.8.key.weight': n_params=32768\n",
      "name='blocks.4.self_attn.heads.8.query.weight': n_params=32768\n",
      "name='blocks.4.self_attn.heads.8.value.weight': n_params=32768\n",
      "name='blocks.4.self_attn.heads.9.key.weight': n_params=32768\n",
      "name='blocks.4.self_attn.heads.9.query.weight': n_params=32768\n",
      "name='blocks.4.self_attn.heads.9.value.weight': n_params=32768\n",
      "name='blocks.4.self_attn.proj.weight': n_params=327680\n",
      "name='blocks.4.self_attn.proj.bias': n_params=512\n",
      "name='blocks.4.ff_ln.weight': n_params=512\n",
      "name='blocks.4.ff_ln.bias': n_params=512\n",
      "name='blocks.4.ff.layers.0.weight': n_params=1048576\n",
      "name='blocks.4.ff.layers.0.bias': n_params=2048\n",
      "name='blocks.4.ff.layers.2.weight': n_params=1048576\n",
      "name='blocks.4.ff.layers.2.bias': n_params=512\n",
      "name='blocks.5.self_attn_ln.weight': n_params=512\n",
      "name='blocks.5.self_attn_ln.bias': n_params=512\n",
      "name='blocks.5.self_attn.heads.0.key.weight': n_params=32768\n",
      "name='blocks.5.self_attn.heads.0.query.weight': n_params=32768\n",
      "name='blocks.5.self_attn.heads.0.value.weight': n_params=32768\n",
      "name='blocks.5.self_attn.heads.1.key.weight': n_params=32768\n",
      "name='blocks.5.self_attn.heads.1.query.weight': n_params=32768\n",
      "name='blocks.5.self_attn.heads.1.value.weight': n_params=32768\n",
      "name='blocks.5.self_attn.heads.2.key.weight': n_params=32768\n",
      "name='blocks.5.self_attn.heads.2.query.weight': n_params=32768\n",
      "name='blocks.5.self_attn.heads.2.value.weight': n_params=32768\n",
      "name='blocks.5.self_attn.heads.3.key.weight': n_params=32768\n",
      "name='blocks.5.self_attn.heads.3.query.weight': n_params=32768\n",
      "name='blocks.5.self_attn.heads.3.value.weight': n_params=32768\n",
      "name='blocks.5.self_attn.heads.4.key.weight': n_params=32768\n",
      "name='blocks.5.self_attn.heads.4.query.weight': n_params=32768\n",
      "name='blocks.5.self_attn.heads.4.value.weight': n_params=32768\n",
      "name='blocks.5.self_attn.heads.5.key.weight': n_params=32768\n",
      "name='blocks.5.self_attn.heads.5.query.weight': n_params=32768\n",
      "name='blocks.5.self_attn.heads.5.value.weight': n_params=32768\n",
      "name='blocks.5.self_attn.heads.6.key.weight': n_params=32768\n",
      "name='blocks.5.self_attn.heads.6.query.weight': n_params=32768\n",
      "name='blocks.5.self_attn.heads.6.value.weight': n_params=32768\n",
      "name='blocks.5.self_attn.heads.7.key.weight': n_params=32768\n",
      "name='blocks.5.self_attn.heads.7.query.weight': n_params=32768\n",
      "name='blocks.5.self_attn.heads.7.value.weight': n_params=32768\n",
      "name='blocks.5.self_attn.heads.8.key.weight': n_params=32768\n",
      "name='blocks.5.self_attn.heads.8.query.weight': n_params=32768\n",
      "name='blocks.5.self_attn.heads.8.value.weight': n_params=32768\n",
      "name='blocks.5.self_attn.heads.9.key.weight': n_params=32768\n",
      "name='blocks.5.self_attn.heads.9.query.weight': n_params=32768\n",
      "name='blocks.5.self_attn.heads.9.value.weight': n_params=32768\n",
      "name='blocks.5.self_attn.proj.weight': n_params=327680\n",
      "name='blocks.5.self_attn.proj.bias': n_params=512\n",
      "name='blocks.5.ff_ln.weight': n_params=512\n",
      "name='blocks.5.ff_ln.bias': n_params=512\n",
      "name='blocks.5.ff.layers.0.weight': n_params=1048576\n",
      "name='blocks.5.ff.layers.0.bias': n_params=2048\n",
      "name='blocks.5.ff.layers.2.weight': n_params=1048576\n",
      "name='blocks.5.ff.layers.2.bias': n_params=512\n",
      "name='blocks.6.self_attn_ln.weight': n_params=512\n",
      "name='blocks.6.self_attn_ln.bias': n_params=512\n",
      "name='blocks.6.self_attn.heads.0.key.weight': n_params=32768\n",
      "name='blocks.6.self_attn.heads.0.query.weight': n_params=32768\n",
      "name='blocks.6.self_attn.heads.0.value.weight': n_params=32768\n",
      "name='blocks.6.self_attn.heads.1.key.weight': n_params=32768\n",
      "name='blocks.6.self_attn.heads.1.query.weight': n_params=32768\n",
      "name='blocks.6.self_attn.heads.1.value.weight': n_params=32768\n",
      "name='blocks.6.self_attn.heads.2.key.weight': n_params=32768\n",
      "name='blocks.6.self_attn.heads.2.query.weight': n_params=32768\n",
      "name='blocks.6.self_attn.heads.2.value.weight': n_params=32768\n",
      "name='blocks.6.self_attn.heads.3.key.weight': n_params=32768\n",
      "name='blocks.6.self_attn.heads.3.query.weight': n_params=32768\n",
      "name='blocks.6.self_attn.heads.3.value.weight': n_params=32768\n",
      "name='blocks.6.self_attn.heads.4.key.weight': n_params=32768\n",
      "name='blocks.6.self_attn.heads.4.query.weight': n_params=32768\n",
      "name='blocks.6.self_attn.heads.4.value.weight': n_params=32768\n",
      "name='blocks.6.self_attn.heads.5.key.weight': n_params=32768\n",
      "name='blocks.6.self_attn.heads.5.query.weight': n_params=32768\n",
      "name='blocks.6.self_attn.heads.5.value.weight': n_params=32768\n",
      "name='blocks.6.self_attn.heads.6.key.weight': n_params=32768\n",
      "name='blocks.6.self_attn.heads.6.query.weight': n_params=32768\n",
      "name='blocks.6.self_attn.heads.6.value.weight': n_params=32768\n",
      "name='blocks.6.self_attn.heads.7.key.weight': n_params=32768\n",
      "name='blocks.6.self_attn.heads.7.query.weight': n_params=32768\n",
      "name='blocks.6.self_attn.heads.7.value.weight': n_params=32768\n",
      "name='blocks.6.self_attn.heads.8.key.weight': n_params=32768\n",
      "name='blocks.6.self_attn.heads.8.query.weight': n_params=32768\n",
      "name='blocks.6.self_attn.heads.8.value.weight': n_params=32768\n",
      "name='blocks.6.self_attn.heads.9.key.weight': n_params=32768\n",
      "name='blocks.6.self_attn.heads.9.query.weight': n_params=32768\n",
      "name='blocks.6.self_attn.heads.9.value.weight': n_params=32768\n",
      "name='blocks.6.self_attn.proj.weight': n_params=327680\n",
      "name='blocks.6.self_attn.proj.bias': n_params=512\n",
      "name='blocks.6.ff_ln.weight': n_params=512\n",
      "name='blocks.6.ff_ln.bias': n_params=512\n",
      "name='blocks.6.ff.layers.0.weight': n_params=1048576\n",
      "name='blocks.6.ff.layers.0.bias': n_params=2048\n",
      "name='blocks.6.ff.layers.2.weight': n_params=1048576\n",
      "name='blocks.6.ff.layers.2.bias': n_params=512\n",
      "name='blocks.7.self_attn_ln.weight': n_params=512\n",
      "name='blocks.7.self_attn_ln.bias': n_params=512\n",
      "name='blocks.7.self_attn.heads.0.key.weight': n_params=32768\n",
      "name='blocks.7.self_attn.heads.0.query.weight': n_params=32768\n",
      "name='blocks.7.self_attn.heads.0.value.weight': n_params=32768\n",
      "name='blocks.7.self_attn.heads.1.key.weight': n_params=32768\n",
      "name='blocks.7.self_attn.heads.1.query.weight': n_params=32768\n",
      "name='blocks.7.self_attn.heads.1.value.weight': n_params=32768\n",
      "name='blocks.7.self_attn.heads.2.key.weight': n_params=32768\n",
      "name='blocks.7.self_attn.heads.2.query.weight': n_params=32768\n",
      "name='blocks.7.self_attn.heads.2.value.weight': n_params=32768\n",
      "name='blocks.7.self_attn.heads.3.key.weight': n_params=32768\n",
      "name='blocks.7.self_attn.heads.3.query.weight': n_params=32768\n",
      "name='blocks.7.self_attn.heads.3.value.weight': n_params=32768\n",
      "name='blocks.7.self_attn.heads.4.key.weight': n_params=32768\n",
      "name='blocks.7.self_attn.heads.4.query.weight': n_params=32768\n",
      "name='blocks.7.self_attn.heads.4.value.weight': n_params=32768\n",
      "name='blocks.7.self_attn.heads.5.key.weight': n_params=32768\n",
      "name='blocks.7.self_attn.heads.5.query.weight': n_params=32768\n",
      "name='blocks.7.self_attn.heads.5.value.weight': n_params=32768\n",
      "name='blocks.7.self_attn.heads.6.key.weight': n_params=32768\n",
      "name='blocks.7.self_attn.heads.6.query.weight': n_params=32768\n",
      "name='blocks.7.self_attn.heads.6.value.weight': n_params=32768\n",
      "name='blocks.7.self_attn.heads.7.key.weight': n_params=32768\n",
      "name='blocks.7.self_attn.heads.7.query.weight': n_params=32768\n",
      "name='blocks.7.self_attn.heads.7.value.weight': n_params=32768\n",
      "name='blocks.7.self_attn.heads.8.key.weight': n_params=32768\n",
      "name='blocks.7.self_attn.heads.8.query.weight': n_params=32768\n",
      "name='blocks.7.self_attn.heads.8.value.weight': n_params=32768\n",
      "name='blocks.7.self_attn.heads.9.key.weight': n_params=32768\n",
      "name='blocks.7.self_attn.heads.9.query.weight': n_params=32768\n",
      "name='blocks.7.self_attn.heads.9.value.weight': n_params=32768\n",
      "name='blocks.7.self_attn.proj.weight': n_params=327680\n",
      "name='blocks.7.self_attn.proj.bias': n_params=512\n",
      "name='blocks.7.ff_ln.weight': n_params=512\n",
      "name='blocks.7.ff_ln.bias': n_params=512\n",
      "name='blocks.7.ff.layers.0.weight': n_params=1048576\n",
      "name='blocks.7.ff.layers.0.bias': n_params=2048\n",
      "name='blocks.7.ff.layers.2.weight': n_params=1048576\n",
      "name='blocks.7.ff.layers.2.bias': n_params=512\n",
      "name='f_ln.weight': n_params=512\n",
      "name='f_ln.bias': n_params=512\n",
      "name='out.weight': n_params=33280\n",
      "name='out.bias': n_params=65\n",
      "\n",
      "27.633729 M parameters total\n",
      "\n",
      "Generating sample...\n",
      "\n",
      "d\n",
      "OgggC;fgzbz;C'C&Tgg.\n",
      "B.z&\n",
      "ddzFa&.q&C\n",
      "S&O:xTOgg\n",
      "!HgVVKOgggO&YSOxK&RpOw&&CgM\n",
      "KIT!$YoCgV\n",
      "&\n",
      "&&j.Vu.W&zcgi\n",
      "&iVK&TM\n",
      "zR&$P&&VH\n",
      "&&zECFigj&Tz\n",
      "V&h\n",
      "h&&gU.HSH.g\n",
      "\n",
      "VZjTVKgpgHSxMYOgTK;V-HV.SVj.&\n",
      "xiVT&-OTVwu&.Ehzc."
     ]
    }
   ],
   "source": [
    "\"\"\"Build model, view its layers and parameters, and sample generation.\"\"\"\n",
    "\n",
    "nanogpt = NanoGPT().to(device)\n",
    "\n",
    "print(nanogpt)\n",
    "n_params_tot = 0\n",
    "for name, parameter in nanogpt.named_parameters():\n",
    "    if not parameter.requires_grad:\n",
    "        continue\n",
    "    n_params = parameter.numel()\n",
    "    print(f\"{name=}: {n_params=}\")\n",
    "    n_params_tot += n_params\n",
    "print(f\"\\n{n_params_tot / 1e6} M parameters total\\n\")\n",
    "\n",
    "print(\"Generating sample...\\n\")\n",
    "in_txt = (\n",
    "    \"Wherefore art thou, Romeo? \"\n",
    "    \"We are such stuff as dreams are made on. \" \n",
    "    \"The course of true love never did run smooth.\"\n",
    ")\n",
    "gen = generate(nanogpt, tokens, in_txt=in_txt, n_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create a DataLoader.\"\"\"\n",
    "\n",
    "def build_dataset(txtfile, ctx_len):\n",
    "    \"\"\"Build dataset from text file.\"\"\"\n",
    "    with open(txtfile) as f:\n",
    "        text = f.read()\n",
    "    tokens = sorted(list(set(text)))\n",
    "    token_to_int = {t: i for i, t in enumerate(tokens)}\n",
    "    encode = lambda tokens: [token_to_int[t] for t in tokens]\n",
    "    data = torch.tensor(encode(text), dtype=torch.long)\n",
    "    n_chars = len(text)\n",
    "    n_examples = n_chars - ctx_len\n",
    "    idxs = torch.arange(ctx_len + 1).unsqueeze(0) + torch.arange(n_examples).unsqueeze(1)\n",
    "    X, Y = data[idxs[:, :-1]], data[idxs[:, 1:]]\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "X, Y = build_dataset(Path.cwd() / \"data/tiny_shakespeare.txt\", nanogpt.ctx_len)\n",
    "dataset = TensorDataset(X, Y)\n",
    "train_data, test_data, val_data = random_split(dataset, [0.9, 0.05, 0.05])\n",
    "batch_sz = 32\n",
    "train_loader = DataLoader(train_data, batch_size=batch_sz, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_sz, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_sz, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create a train function.\"\"\"\n",
    "\n",
    "def apply_gradient_centralization(optimizer):\n",
    "    \"\"\"Applies gradient centralization to the optimizer.\n",
    "\n",
    "    This function should be called before optimizer.step() in the training loop.\n",
    "    \"\"\"\n",
    "    for group in optimizer.param_groups:\n",
    "        for param in group[\"params\"]:\n",
    "            if param.grad is not None:\n",
    "                # Compute the mean of the gradient\n",
    "                grad_mean = param.grad.data.mean(dim=tuple(range(1, len(param.grad.shape))), keepdim=True)\n",
    "                # Centralize the gradient\n",
    "                param.grad.data -= grad_mean\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: nn.Module,  # model\n",
    "    train_loader: DataLoader,  # batched dataset for training\n",
    "    val_loader: DataLoader,  # batched dataset for validation\n",
    "    optimizer: optim,  # optimizer\n",
    "    loss_fn: nn.modules.loss,  # loss function\n",
    "    max_epochs: int = 2,  # max n training epochs\n",
    "    max_batches: int = 1e9,  # max n batches to train\n",
    "    val_chk_interval: int = 200,  # check val loss every `val_chk_interval` batches and print losses\n",
    "    val_iter: int = 5,  # number of batches on val_loader to run and avg when computing val loss\n",
    "    patience_thresh: int = 1e9,  # consecutive batches without val loss decrease for early stopping\n",
    "    save_chkpt_dir: str = \"\",  # dir to save model checkpoint\n",
    "    save_chkpt_thresh: float = 0.5,  # save model checkpoint every `save_chkpt_interval` loss decrease\n",
    ") -> tuple[torch.Tensor, np.ndarray, np.ndarray]:  # -> loss, train_losses, val_losses\n",
    "    \"\"\"Trains a model, returns loss.\"\"\"\n",
    "    # <s Nested helper functions to make `train` more readable.\n",
    "    def print_losses(epoch, batch_i, train_losses_avg, val_losses_avg):\n",
    "        \"\"\"Print current average losses.\"\"\"\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}: Batch {batch_i + 1}:  \"\n",
    "            f\"Loss = {train_losses_avg[-1]:.3f}, Val Loss = {val_losses_avg[-1]:.3f}\"\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def estimate_losses(model, val_loader, val_losses, val_losses_avg, train_losses, train_losses_avg):\n",
    "        \"\"\"Estimate losses on val_loader, and return val loss and train loss avg.\"\"\"\n",
    "        model.eval()\n",
    "        for val_i, (x_val, y_val) in enumerate(val_loader):\n",
    "            logits = model(x_val.to(device))\n",
    "            val_loss = loss_fn(logits.view(-1, n_tokens), y_val.to(device).view(-1))\n",
    "            val_losses.append(val_loss.item())\n",
    "            if val_i >= (val_iter - 1):\n",
    "                break\n",
    "        val_losses_avg.append(np.mean(val_losses[-val_iter:]))\n",
    "        train_losses_avg.append(np.mean(train_losses[-val_chk_interval:]))\n",
    "        model.train()\n",
    "    # /s>\n",
    "\n",
    "    # <s Trackers\n",
    "    ctx_len, n_tokens  = model.ctx_len, model.n_tokens\n",
    "    batch_sz, n_batches = train_loader.batch_size, len(train_loader)\n",
    "    batch_lim = min(max_batches, n_batches * max_epochs)\n",
    "    patience_thresh *= val_chk_interval  # convert to batches within model validation block\n",
    "    train_losses, val_losses, train_losses_avg, val_losses_avg = [], [], [], []\n",
    "    init_loss, best_val_loss = float(\"inf\"), float(\"inf\")\n",
    "    patience_ct = 0\n",
    "    # /s>\n",
    "\n",
    "    # <s Training loop\n",
    "    for epoch in range(max_epochs):\n",
    "        pbar = tqdm(enumerate(train_loader), total=batch_lim, desc=\"Batch progression\")  # tqdm progress bar\n",
    "        for batch_i, (x_train, y_train) in pbar:\n",
    "            # <ss Model training.\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x_train.to(device))  # -> [batch_sz, ctx_len, n_tokens], but...\n",
    "            # must reshape to compare against batch_sz vector of targets for cross-entropy loss.\n",
    "            loss = loss_fn(logits.view(-1, n_tokens), y_train.to(device).view(-1))\n",
    "            loss.backward()\n",
    "            apply_gradient_centralization(optimizer)\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "            # /ss>\n",
    "            # <ss Model validation.\n",
    "            if val_chk_interval and batch_i % val_chk_interval == 0:\n",
    "                # Estimate and print losses.\n",
    "                estimate_losses(\n",
    "                    model, val_loader, val_losses, val_losses_avg, train_losses, train_losses_avg\n",
    "                )\n",
    "                print_losses(epoch, batch_i, train_losses_avg, val_losses_avg)\n",
    "                pbar.set_postfix_str(f\"Total Batch {(batch_i + 1) * (epoch + 1)} / {batch_lim}\")\n",
    "                # Patience check for early stopping.\n",
    "                patience_ct = (\n",
    "                    0 if val_losses_avg[-1] < best_val_loss else patience_ct + val_chk_interval\n",
    "                )\n",
    "                best_val_loss = min(best_val_loss, val_losses_avg[-1])\n",
    "                if patience_ct >= patience_thresh:\n",
    "                    print(\"Early stopping.\")\n",
    "                    print_losses(epoch, batch_i, train_losses_avg, val_losses_avg)\n",
    "                    return loss, train_losses_avg, val_losses_avg\n",
    "            # Max batch check.\n",
    "            if (batch_i + 1) * (epoch + 1) >= max_batches:\n",
    "                print(\"Finished training:\")\n",
    "                print_losses(epoch, batch_i, train_losses_avg, val_losses_avg)\n",
    "                return loss, train_losses_avg, val_losses_avg\n",
    "            # Save checkpoint check.\n",
    "            if (Path(save_chkpt_dir).exists()) and (init_loss - loss.item() > save_chkpt_thresh):\n",
    "                torch.save(model.state_dict(), Path(save_chkpt_dir) / f\"model_chkpt_loss{loss.item():.3f}.pth\")\n",
    "                init_loss = loss.item()\n",
    "            # /ss> /s>\n",
    "\n",
    "    print(\"Finished training:\")\n",
    "    print_losses(epoch, batch_i, train_losses_avg, val_losses_avg)\n",
    "    return loss, train_losses_avg, val_losses_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch progression:   0%|          | 1/1000 [00:01<29:12,  1.75s/it, Total Batch 1 / 1000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Batch 1:  Loss = 5.083, Val Loss = 5.909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch progression:  20%|██        | 201/1000 [01:30<09:18,  1.43it/s, Total Batch 201 / 1000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Batch 201:  Loss = 2.982, Val Loss = 2.504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch progression:  40%|████      | 401/1000 [02:59<06:46,  1.47it/s, Total Batch 401 / 1000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Batch 401:  Loss = 2.500, Val Loss = 2.436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch progression:  60%|██████    | 601/1000 [04:27<04:35,  1.45it/s, Total Batch 601 / 1000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Batch 601:  Loss = 2.413, Val Loss = 2.318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch progression:  80%|████████  | 801/1000 [05:56<02:19,  1.43it/s, Total Batch 801 / 1000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Batch 801:  Loss = 2.236, Val Loss = 2.036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch progression: 100%|█████████▉| 999/1000 [07:22<00:00,  2.26it/s, Total Batch 801 / 1000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training:\n",
      "Epoch 1: Batch 1000:  Loss = 2.236, Val Loss = 2.036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Train and eval for just a few batches.\"\"\"\n",
    "\n",
    "save_chkpt_dir = Path.cwd() / \"models/shakespeare_chkpts\"\n",
    "adam = torch.optim.AdamW(nanogpt.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss, train_losses, val_losses = train(\n",
    "    nanogpt, train_loader, val_loader, adam, loss_fn, max_batches=1000, save_chkpt_dir=save_chkpt_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "KING RICHARD IIIII:\n",
      "Why, hould come to him word the worldow,\n",
      "The man so for the crion of your buther.\n",
      "\n",
      "KING HENRY VI:\n",
      "Not a more so the will the dough all the see.\n",
      "\n",
      "GLOUCESTER:\n",
      "Nor what more the con"
     ]
    }
   ],
   "source": [
    "\"\"\"Post-training generation.\"\"\"\n",
    "\n",
    "in_txt = (\n",
    "    \"Wherefore art thou, Romeo? \"\n",
    "    \"We are such stuff as dreams are made on. \"\n",
    "    \"The course of true love never did run smooth.\"\n",
    ")\n",
    "\n",
    "gen = generate(nanogpt, tokens, in_txt=in_txt, n_tokens=200, top_k=50, temp=0.5, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jai\\AppData\\Local\\Temp\\ipykernel_41960\\2513295627.py:37: UserWarning: Ratio of n_heads and head_sz to emb_dim 0.8 is not 1\n",
      "  warn(f\"Ratio of n_heads and head_sz to emb_dim {emb_dim / n_heads / head_sz} is not 1\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Save / load model.\"\"\"\n",
    "\n",
    "# Save\n",
    "torch.save(nanogpt.state_dict(), Path.cwd() / \"models/nanogpt_shakespeare.pth\")\n",
    "with open(Path.cwd() / \"models/nanogpt_shakespeare_config.json\", \"w\") as f:\n",
    "    json.dump(model_config, f)\n",
    "\n",
    "# Wait a sec, then try and load\n",
    "time.sleep(1)\n",
    "with open(Path.cwd() / \"models/nanogpt_shakespeare_config.json\", \"w\") as f:\n",
    "    model_config = json.load(f)\n",
    "nanogpt = NanoGPT(\n",
    "    n_tokens=model_config[\"n_tokens\"],\n",
    "    ctx_len=model_config[\"ctx_len\"],\n",
    "    n_blocks=model_config[\"n_blocks\"],\n",
    "    n_heads=model_config[\"n_heads\"],\n",
    "    head_sz=model_config[\"head_sz\"],\n",
    "    emb_dim=model_config[\"emb_dim\"],\n",
    "    ff_dim=model_config[\"ff_dim\"],\n",
    "    dropout=model_config[\"dropout\"],\n",
    ").to(device)\n",
    "nanogpt.load_state_dict(torch.load((Path.cwd() / \"models/nanogpt_shakespeare.pth\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro2dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
